{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f4de777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ V.4 (‡πÇ‡∏´‡∏•‡∏î, ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î, ‡∏£‡∏ß‡∏°, ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å) ---\n",
      "üìñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Ç‡∏ï: ../data/PROCESSED/overall_data.xlsx (‡∏ä‡∏µ‡∏ï: districts)\n",
      "   üó∫Ô∏è ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏Ç‡∏ï (join_key V4) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß\n",
      "üåä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏•‡∏≠‡∏á: ../data/RAW/canel_list.xls (‡∏ä‡∏µ‡∏ï: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏•‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)\n",
      "   üåä ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡∏•‡∏≠‡∏á (join_key V4) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß\n",
      "\n",
      "==================================================\n",
      "ü§ù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏•‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡∏ï (‡∏î‡πâ‡∏ß‡∏¢ join_key V4)...\n",
      "‚úÖ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏≤ DCODE ‡πÑ‡∏°‡πà‡∏û‡∏ö: 0\n",
      "\n",
      "==================================================\n",
      "üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå...\n",
      "üéâ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\n",
      "   ‡πÑ‡∏ü‡∏•‡πå: canals_with_dcode.csv\n",
      "   ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà: ../data/PROCESSED/canals_with_dcode.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Regular Expressions\n",
    "\n",
    "# --- 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå ---\n",
    "\n",
    "# Path ‡πÑ‡∏ü‡∏•‡πå Input (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö)\n",
    "canal_file_path = '../data/RAW/canel_list.xls'\n",
    "district_file_path = '../data/PROCESSED/overall_data.xlsx'\n",
    "\n",
    "# Path ‡πÑ‡∏ü‡∏•‡πå Output (‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å)\n",
    "output_dir = '../data/PROCESSED/'\n",
    "output_filename = 'canals_with_dcode.csv'\n",
    "output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "# ‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏µ‡∏ï‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "canal_sheet_name = '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏•‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£'\n",
    "district_sheet_name = 'districts'\n",
    "\n",
    "print(\"--- üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ V.4 (‡πÇ‡∏´‡∏•‡∏î, ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î, ‡∏£‡∏ß‡∏°, ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å) ---\")\n",
    "\n",
    "\n",
    "# --- 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î V4 (‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô) ---\n",
    "def normalize_district_name_v4(name):\n",
    "    \"\"\"\n",
    "    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô V4: ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "    1. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô string\n",
    "    2. ‡∏•‡∏ö \"‡πÄ‡∏Ç‡∏ï\"\n",
    "    3. ‡∏•‡∏ö Non-breaking space (u'\\xa0')\n",
    "    4. ‡πÅ‡∏Å‡πâ‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î '‡∏£‡∏≤‡∏©‡∏è‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞' (‡∏è) -> '‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞' (‡∏é)\n",
    "    5. ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á \"‡∏ó‡∏∏‡∏Å‡∏ä‡∏ô‡∏¥‡∏î\" ( \\s+ ) ‡∏ó‡∏¥‡πâ‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    \n",
    "    clean_name = str(name)\n",
    "    clean_name = clean_name.replace('‡πÄ‡∏Ç‡∏ï', '')\n",
    "    clean_name = clean_name.replace(u'\\xa0', '')\n",
    "    clean_name = clean_name.replace('‡∏£‡∏≤‡∏©‡∏è‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞', '‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞') # ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î\n",
    "    clean_name = re.sub(r'\\s+', '', clean_name) # ‡∏•‡∏ö space ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "    \n",
    "    return clean_name\n",
    "\n",
    "\n",
    "try:\n",
    "    # --- 3. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡∏ï (Districts) ---\n",
    "    print(f\"üìñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Ç‡∏ï: {district_file_path} (‡∏ä‡∏µ‡∏ï: {district_sheet_name})\")\n",
    "    df_districts = pd.read_excel(district_file_path, sheet_name=district_sheet_name)\n",
    "    df_district_map = df_districts[['dcode', 'dname']].copy()\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á 'join_key' ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô V4\n",
    "    df_district_map['join_key'] = df_district_map['dname'].apply(normalize_district_name_v4)\n",
    "    print(\"   üó∫Ô∏è ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏Ç‡∏ï (join_key V4) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "\n",
    "\n",
    "    # --- 4. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏•‡∏≠‡∏á (Canals) ---\n",
    "    print(f\"üåä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏•‡∏≠‡∏á: {canal_file_path} (‡∏ä‡∏µ‡∏ï: {canal_sheet_name})\")\n",
    "    # header=2 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà 3\n",
    "    df_canals_raw = pd.read_excel(canal_file_path, sheet_name=canal_sheet_name, header=2) \n",
    "    df_canals = df_canals_raw[['‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≠‡∏á', '‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï']].copy()\n",
    "    \n",
    "    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á\n",
    "    df_canals = df_canals.dropna(subset=['‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≠‡∏á', '‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï'])\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á 'join_key' ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô V4 \"‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\"\n",
    "    df_canals['join_key'] = df_canals['‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï'].apply(normalize_district_name_v4)\n",
    "    print(\"   üåä ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡∏•‡∏≠‡∏á (join_key V4) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "\n",
    "\n",
    "    # --- 5. ‡∏£‡∏ß‡∏° (Merge) ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ü§ù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏•‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡∏ï (‡∏î‡πâ‡∏ß‡∏¢ join_key V4)...\")\n",
    "    \n",
    "    df_final = pd.merge(\n",
    "        df_canals,\n",
    "        df_district_map,\n",
    "        on='join_key',  # ‡πÉ‡∏ä‡πâ Key ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô 100%\n",
    "        how='left'      # 'left' = ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏•‡∏≠‡∏á‡πÑ‡∏ß‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "    )\n",
    "    \n",
    "    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤ join_key ‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢)\n",
    "    df_result = df_final[['‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≠‡∏á', '‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï', 'dcode', 'dname']]\n",
    "    \n",
    "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å)\n",
    "    unmatched_count = df_result['dcode'].isna().sum()\n",
    "    print(f\"‚úÖ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏≤ DCODE ‡πÑ‡∏°‡πà‡∏û‡∏ö: {unmatched_count}\")\n",
    "\n",
    "\n",
    "    # --- 6. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå (Save) ---\n",
    "    if unmatched_count == 0:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå...\")\n",
    "        \n",
    "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå PROCESSED (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô CSV\n",
    "        # index=False ‡∏Ñ‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡πÄ‡∏•‡∏Ç index (0,1,2..) ‡∏Ç‡∏≠‡∏á DataFrame ‡πÑ‡∏õ‡πÉ‡∏™‡πà‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå\n",
    "        # encoding='utf-8-sig' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Excel ‡πÄ‡∏õ‡∏¥‡∏î‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
    "        df_result.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"üéâ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "        print(f\"   ‡πÑ‡∏ü‡∏•‡πå: {output_filename}\")\n",
    "        print(f\"   ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà: {output_path}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå [‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô] ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏´‡∏≤ DCODE ‡πÑ‡∏°‡πà‡∏û‡∏ö\")\n",
    "        print(\"   ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\")\n",
    "        unmatched_keys = df_result[df_result['dcode'].isna()]['join_key'].unique()\n",
    "        print(f\"   Join Keys ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠: {unmatched_keys[:20]}\")\n",
    "\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå\")\n",
    "    print(f\"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á: {e.filename}\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏µ‡∏ï (Sheet)\")\n",
    "    if 'Worksheet' in str(e):\n",
    "        print(f\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ä‡∏µ‡∏ï '{canal_sheet_name}' ‡∏´‡∏£‡∏∑‡∏≠ '{district_sheet_name}'\")\n",
    "    else:\n",
    "        print(e)\n",
    "except KeyError as e:\n",
    "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\")\n",
    "    print(f\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏∑‡πà‡∏≠ {e} ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Header ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aeb74c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üó∫Ô∏è ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà DCODE ---\n",
      "‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà DCODE ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏°‡∏µ 50 ‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô)\n",
      "\n",
      "--- üå¶Ô∏è ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô ---\n",
      "üìÇ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô 12 ‡πÑ‡∏ü‡∏•‡πå (‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå Temp ‡πÅ‡∏•‡πâ‡∏ß). ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏ô‡∏•‡∏π‡∏õ...\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 01.xlsx...\n",
      "   ‚úÖ 01.xlsx (‡∏£‡∏ß‡∏° 31 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 02.xlsx...\n",
      "   ‚úÖ 02.xlsx (‡∏£‡∏ß‡∏° 29 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 03.xlsx...\n",
      "   ‚úÖ 03.xlsx (‡∏£‡∏ß‡∏° 31 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 04.xlsx...\n",
      "   ‚úÖ 04.xlsx (‡∏£‡∏ß‡∏° 30 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 05.xlsx...\n",
      "   ‚úÖ 05.xlsx (‡∏£‡∏ß‡∏° 31 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 06.xlsx...\n",
      "   ‚úÖ 06.xlsx (‡∏£‡∏ß‡∏° 30 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 07.xlsx...\n",
      "   ‚úÖ 07.xlsx (‡∏£‡∏ß‡∏° 31 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 08.xlsx...\n",
      "   ‚úÖ 08.xlsx (‡∏£‡∏ß‡∏° 31 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 09.xlsx...\n",
      "   ‚úÖ 09.xlsx (‡∏£‡∏ß‡∏° 30 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 10.xlsx...\n",
      "   ‚úÖ 10.xlsx (‡∏£‡∏ß‡∏° 31 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 11.xlsx...\n",
      "   ‚úÖ 11.xlsx (‡∏£‡∏ß‡∏° 30 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 12.xlsx...\n",
      "   ‚úÖ 12.xlsx (‡∏£‡∏ß‡∏° 31 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "==================================================\n",
      "...‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å 365+ ‡∏ä‡∏µ‡∏ï...\n",
      "‚úÖ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 1.64 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)\n",
      "   ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: 18666 ‡πÅ‡∏ñ‡∏ß, 16 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
      "\n",
      "   ‚ö†Ô∏è ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: ‡∏°‡∏µ 366 ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏´‡∏≤ DCODE ‡πÑ‡∏°‡πà‡∏û‡∏ö\n",
      "   ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö: ['‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£']\n",
      "   [i] (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•) '‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø' ‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß\n",
      "   [i] (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•) '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£' ‡∏ñ‡∏π‡∏Å‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà ‡∏Å‡∏ó‡∏°. (‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)\n",
      "\n",
      "üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏õ‡∏ó‡∏µ‡πà: ../data/PROCESSED/rain_2024_combined_with_dcode.csv\n",
      "üéâ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Regular Expressions\n",
    "import time # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤\n",
    "\n",
    "# --- 1. [V4.2] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏ï (‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î) ---\n",
    "def normalize_district_name_v4_2(name):\n",
    "    \"\"\"\n",
    "    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô V4.2:\n",
    "    1. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠/‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢ (‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø, ‡∏£‡∏≤‡∏©‡∏è‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞)\n",
    "    2. ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÄ‡∏Ç‡∏ï, ‡∏≠‡∏≥‡πÄ‡∏†‡∏≠)\n",
    "    3. ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô (\\xa0)\n",
    "    4. ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏∏‡∏Å‡∏ä‡∏ô‡∏¥‡∏î ( \\s+ ) ‡∏ó‡∏¥‡πâ‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    \n",
    "    clean_name = str(name)\n",
    "    \n",
    "    # --- 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠/‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î ‡∏Å‡πà‡∏≠‡∏ô ---\n",
    "    clean_name = clean_name.replace('‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø', '‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡πà‡∏≤‡∏¢') # ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠\n",
    "    clean_name = clean_name.replace('‡∏£‡∏≤‡∏©‡∏è‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞', '‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞')    # ‡πÅ‡∏Å‡πâ ‡∏è -> ‡∏é\n",
    "    \n",
    "    # --- 2. ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤ ---\n",
    "    clean_name = clean_name.replace('‡πÄ‡∏Ç‡∏ï', '')\n",
    "    clean_name = clean_name.replace('‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '') # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å ‡∏Å‡∏ó‡∏°.\n",
    "    \n",
    "    # --- 3. ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô ---\n",
    "    clean_name = clean_name.replace(u'\\xa0', '')\n",
    "    \n",
    "    # --- 4. ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ---\n",
    "    clean_name = re.sub(r'\\s+', '', clean_name)\n",
    "    \n",
    "    return clean_name\n",
    "\n",
    "# --- 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° \"‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï\" (District Map) ---\n",
    "print(\"--- üó∫Ô∏è ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà DCODE ---\")\n",
    "try:\n",
    "    district_file_path = '../data/PROCESSED/overall_data.xlsx'\n",
    "    district_sheet_name = 'districts'\n",
    "    \n",
    "    df_districts = pd.read_excel(district_file_path, sheet_name=district_sheet_name)\n",
    "    df_district_map = df_districts[['dcode', 'dname']].copy()\n",
    "    \n",
    "    # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô V4.2 ‡πÉ‡∏´‡∏°‡πà\n",
    "    df_district_map['join_key'] = df_district_map['dname'].apply(normalize_district_name_v4_2)\n",
    "    df_district_map = df_district_map.drop_duplicates(subset=['join_key'])\n",
    "    \n",
    "    print(f\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà DCODE ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏°‡∏µ {len(df_district_map)} ‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î dcode map: {e}\")\n",
    "    # (‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à)\n",
    "    raise e\n",
    "\n",
    "\n",
    "# --- 3. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏ù‡∏ô 12 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ---\n",
    "print(\"\\n--- üå¶Ô∏è ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô ---\")\n",
    "\n",
    "rain_data_dir = '../data/RAW/rain_2024/'\n",
    "\n",
    "# 1. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå .xlsx ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå temp)\n",
    "all_xlsx_files = glob.glob(os.path.join(rain_data_dir, '*.xlsx'))\n",
    "\n",
    "# 2. [V4.1] ‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå temporary ‡∏Ç‡∏≠‡∏á Excel (‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ ~$) ‡∏≠‡∏≠‡∏Å\n",
    "rain_files = sorted(\n",
    "    [f for f in all_xlsx_files if not os.path.basename(f).startswith('~$')]\n",
    ")\n",
    "\n",
    "if not rain_files:\n",
    "    print(f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå .xlsx ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {rain_data_dir}\")\n",
    "else:\n",
    "    print(f\"üìÇ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô {len(rain_files)} ‡πÑ‡∏ü‡∏•‡πå (‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå Temp ‡πÅ‡∏•‡πâ‡∏ß). ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏ô‡∏•‡∏π‡∏õ...\")\n",
    "\n",
    "\n",
    "all_processed_data = [] # ‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö DataFrame ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Loop 1: ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÑ‡∏ü‡∏•‡πå (‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß)\n",
    "    for file_path in rain_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        print(f\"\\n   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: {file_name}...\")\n",
    "        \n",
    "        dfs_by_day = pd.read_excel(file_path, sheet_name=None)\n",
    "        \n",
    "        # Loop 2: ‡∏ß‡∏ô‡∏•‡∏π‡∏õ ~30 ‡∏ä‡∏µ‡∏ï (‡∏ß‡∏±‡∏ô)\n",
    "        for sheet_name, df_day in dfs_by_day.items():\n",
    "            \n",
    "            if df_day.empty or '‡πÄ‡∏Ç‡∏ï' not in df_day.columns:\n",
    "                print(f\"      - (‡∏Ç‡πâ‡∏≤‡∏°) ‡∏ä‡∏µ‡∏ï '{sheet_name}' ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡πÄ‡∏Ç‡∏ï'\")\n",
    "                continue\n",
    "            \n",
    "            # --- Transform & Merge ---\n",
    "            # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô V4.2 ‡πÉ‡∏´‡∏°‡πà\n",
    "            df_day['join_key'] = df_day['‡πÄ‡∏Ç‡∏ï'].apply(normalize_district_name_v4_2)\n",
    "            \n",
    "            df_merged = pd.merge(\n",
    "                df_day,\n",
    "                df_district_map,\n",
    "                on='join_key',\n",
    "                how='left'\n",
    "            )\n",
    "            all_processed_data.append(df_merged)\n",
    "            \n",
    "        print(f\"   ‚úÖ {file_name} (‡∏£‡∏ß‡∏° {len(dfs_by_day)} ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
    "\n",
    "\n",
    "    # --- 4. ‡∏£‡∏ß‡∏°‡∏£‡πà‡∏≤‡∏á (Concat) ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ---\n",
    "    if not all_processed_data:\n",
    "        print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏î‡πÜ ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡πÄ‡∏•‡∏¢!\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"...‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å 365+ ‡∏ä‡∏µ‡∏ï...\")\n",
    "        \n",
    "        final_rain_df = pd.concat(all_processed_data, ignore_index=True)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"‚úÖ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ {end_time - start_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)\")\n",
    "        print(f\"   ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {final_rain_df.shape[0]} ‡πÅ‡∏ñ‡∏ß, {final_rain_df.shape[1]} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\")\n",
    "\n",
    "        # --- 5. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ---\n",
    "        unmatched_rows = final_rain_df[final_rain_df['dcode'].isna()]\n",
    "        unmatched_count = len(unmatched_rows)\n",
    "        \n",
    "        if unmatched_count > 0:\n",
    "            print(f\"\\n   ‚ö†Ô∏è ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: ‡∏°‡∏µ {unmatched_count} ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏´‡∏≤ DCODE ‡πÑ‡∏°‡πà‡∏û‡∏ö\")\n",
    "            unmatched_districts = unmatched_rows['‡πÄ‡∏Ç‡∏ï'].unique()\n",
    "            print(f\"   ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö: {unmatched_districts[:10]}\")\n",
    "            \n",
    "            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ '‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø' ‡∏¢‡∏±‡∏á‡∏´‡∏•‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
    "            if any('‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø' in s for s in unmatched_districts if isinstance(s, str)):\n",
    "                 print(\"   [!] ‡∏¢‡∏±‡∏á‡∏û‡∏ö '‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø' ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç!\")\n",
    "            else:\n",
    "                 print(\"   [i] (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•) '‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø' ‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "                 \n",
    "            # ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà\n",
    "            if any('‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£' in s for s in unmatched_districts if isinstance(s, str)):\n",
    "                print(\"   [i] (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•) '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£' ‡∏ñ‡∏π‡∏Å‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà ‡∏Å‡∏ó‡∏°. (‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)\")\n",
    "\n",
    "        else:\n",
    "            print(\"\\n   ‚úÖ DCODE ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!\")\n",
    "            \n",
    "        # --- 6. ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå ---\n",
    "        if 'dcode' in final_rain_df.columns:\n",
    "            key_cols = ['dcode', 'dname', '‡πÄ‡∏Ç‡∏ï', '‡∏£‡∏´‡∏±‡∏™‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ', '‡∏ß‡∏±‡∏ô-‡πÄ‡∏ß‡∏•‡∏≤']\n",
    "            other_cols = [col for col in final_rain_df.columns if col not in key_cols and col != 'join_key']\n",
    "            final_rain_df_sorted = final_rain_df[key_cols + other_cols]\n",
    "\n",
    "        output_dir = '../data/PROCESSED/'\n",
    "        output_filename = 'rain_2024_combined_with_dcode.csv'\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nüíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏õ‡∏ó‡∏µ‡πà: {output_path}\")\n",
    "        final_rain_df_sorted.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(\"üéâ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b621c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üó∫Ô∏è ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà DCODE ---\n",
      "‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà DCODE ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏°‡∏µ 50 ‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô)\n",
      "\n",
      "--- üå¶Ô∏è ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô ---\n",
      "üìÇ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô 12 ‡πÑ‡∏ü‡∏•‡πå (‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå Temp ‡πÅ‡∏•‡πâ‡∏ß). ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏ô‡∏•‡∏π‡∏õ...\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 01.xlsx...\n",
      "   ‚úÖ 01.xlsx (‡∏£‡∏ß‡∏° 31 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 02.xlsx...\n",
      "   ‚úÖ 02.xlsx (‡∏£‡∏ß‡∏° 29 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 03.xlsx...\n",
      "   ‚úÖ 03.xlsx (‡∏£‡∏ß‡∏° 31 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 04.xlsx...\n",
      "   ‚úÖ 04.xlsx (‡∏£‡∏ß‡∏° 30 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 05.xlsx...\n",
      "   ‚úÖ 05.xlsx (‡∏£‡∏ß‡∏° 31 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 06.xlsx...\n",
      "   ‚úÖ 06.xlsx (‡∏£‡∏ß‡∏° 30 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 07.xlsx...\n",
      "   ‚úÖ 07.xlsx (‡∏£‡∏ß‡∏° 31 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 08.xlsx...\n",
      "   ‚úÖ 08.xlsx (‡∏£‡∏ß‡∏° 31 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 09.xlsx...\n",
      "   ‚úÖ 09.xlsx (‡∏£‡∏ß‡∏° 30 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 10.xlsx...\n",
      "   ‚úÖ 10.xlsx (‡∏£‡∏ß‡∏° 31 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 11.xlsx...\n",
      "   ‚úÖ 11.xlsx (‡∏£‡∏ß‡∏° 30 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 12.xlsx...\n",
      "   ‚úÖ 12.xlsx (‡∏£‡∏ß‡∏° 31 ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "==================================================\n",
      "...‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å 365+ ‡∏ä‡∏µ‡∏ï...\n",
      "‚úÖ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 1.53 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)\n",
      "   ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö (‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏£‡∏≠‡∏á): 18666 ‡πÅ‡∏ñ‡∏ß\n",
      "\n",
      "--- üóëÔ∏è ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà ‡∏Å‡∏ó‡∏°. ---\n",
      "   ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£' ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ 366 ‡πÅ‡∏ñ‡∏ß\n",
      "   ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡∏Å‡∏ó‡∏°.): 18300 ‡πÅ‡∏ñ‡∏ß\n",
      "\n",
      "   ‚úÖ DCODE ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡πÉ‡∏ô ‡∏Å‡∏ó‡∏°.) ‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!\n",
      "   - [V.7] ‡πÅ‡∏õ‡∏•‡∏á dcode ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏° (int) ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
      "\n",
      "   [i] ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•' ‡πÅ‡∏•‡πâ‡∏ß\n",
      "\n",
      "üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡∏Å‡∏ó‡∏°.) ‡πÑ‡∏õ‡∏ó‡∏µ‡πà: ../data/PROCESSED/rain_2024_combined_bkk_only.csv\n",
      "üéâ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Regular Expressions\n",
    "import time # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤\n",
    "\n",
    "# --- 1. [V4.2] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏ï (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°) ---\n",
    "def normalize_district_name_v4_2(name):\n",
    "    \"\"\"\n",
    "    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô V4.2:\n",
    "    1. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠/‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î (‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø, ‡∏£‡∏≤‡∏©‡∏è‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞)\n",
    "    2. ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏Ç‡∏ï, ‡∏≠‡∏≥‡πÄ‡∏†‡∏≠)\n",
    "    3. ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô (\\xa0)\n",
    "    4. ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏∏‡∏Å‡∏ä‡∏ô‡∏¥‡∏î ( \\s+ ) ‡∏ó‡∏¥‡πâ‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    \n",
    "    clean_name = str(name)\n",
    "    clean_name = clean_name.replace('‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø', '‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡πà‡∏≤‡∏¢') # ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠\n",
    "    clean_name = clean_name.replace('‡∏£‡∏≤‡∏©‡∏è‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞', '‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞')    # ‡πÅ‡∏Å‡πâ ‡∏è -> ‡∏é\n",
    "    clean_name = clean_name.replace('‡πÄ‡∏Ç‡∏ï', '')\n",
    "    clean_name = clean_name.replace('‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '') # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å ‡∏Å‡∏ó‡∏°.\n",
    "    clean_name = clean_name.replace(u'\\xa0', '')\n",
    "    clean_name = re.sub(r'\\s+', '', clean_name)\n",
    "    \n",
    "    return clean_name\n",
    "\n",
    "# --- 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° \"‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï\" (District Map) ---\n",
    "print(\"--- üó∫Ô∏è ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà DCODE ---\")\n",
    "try:\n",
    "    district_file_path = '../data/PROCESSED/overall_data.xlsx'\n",
    "    district_sheet_name = 'districts'\n",
    "    \n",
    "    df_districts = pd.read_excel(district_file_path, sheet_name=district_sheet_name)\n",
    "    df_district_map = df_districts[['dcode', 'dname']].copy()\n",
    "    \n",
    "    df_district_map['join_key'] = df_district_map['dname'].apply(normalize_district_name_v4_2)\n",
    "    df_district_map = df_district_map.drop_duplicates(subset=['join_key'])\n",
    "    \n",
    "    print(f\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà DCODE ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏°‡∏µ {len(df_district_map)} ‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î dcode map: {e}\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "# --- 3. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏ù‡∏ô 12 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ---\n",
    "print(\"\\n--- üå¶Ô∏è ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô ---\")\n",
    "\n",
    "rain_data_dir = '../data/RAW/rain_2024/'\n",
    "all_xlsx_files = glob.glob(os.path.join(rain_data_dir, '*.xlsx'))\n",
    "\n",
    "# ‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå temporary ‡∏Ç‡∏≠‡∏á Excel (‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ ~$) ‡∏≠‡∏≠‡∏Å\n",
    "rain_files = sorted(\n",
    "    [f for f in all_xlsx_files if not os.path.basename(f).startswith('~$')]\n",
    ")\n",
    "\n",
    "if not rain_files:\n",
    "    print(f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå .xlsx ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡∏≠‡∏£‡πå: {rain_data_dir}\")\n",
    "else:\n",
    "    print(f\"üìÇ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô {len(rain_files)} ‡πÑ‡∏ü‡∏•‡πå (‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå Temp ‡πÅ‡∏•‡πâ‡∏ß). ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏ô‡∏•‡∏π‡∏õ...\")\n",
    "\n",
    "all_processed_data = []\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Loop 1: ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÑ‡∏ü‡∏•‡πå\n",
    "    for file_path in rain_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        print(f\"\\n   ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: {file_name}...\")\n",
    "        \n",
    "        dfs_by_day = pd.read_excel(file_path, sheet_name=None)\n",
    "        \n",
    "        # Loop 2: ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ä‡∏µ‡∏ï\n",
    "        for sheet_name, df_day in dfs_by_day.items():\n",
    "            if df_day.empty or '‡πÄ‡∏Ç‡∏ï' not in df_day.columns:\n",
    "                print(f\"      - (‡∏Ç‡πâ‡∏≤‡∏°) ‡∏ä‡∏µ‡∏ï '{sheet_name}' ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡πÄ‡∏Ç‡∏ï'\")\n",
    "                continue\n",
    "            \n",
    "            # Transform & Merge\n",
    "            df_day['join_key'] = df_day['‡πÄ‡∏Ç‡∏ï'].apply(normalize_district_name_v4_2)\n",
    "            df_merged = pd.merge(\n",
    "                df_day, df_district_map, on='join_key', how='left'\n",
    "            )\n",
    "            all_processed_data.append(df_merged)\n",
    "            \n",
    "        print(f\"   ‚úÖ {file_name} (‡∏£‡∏ß‡∏° {len(dfs_by_day)} ‡∏ä‡∏µ‡∏ï) ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
    "\n",
    "\n",
    "    # --- 4. ‡∏£‡∏ß‡∏°‡∏£‡πà‡∏≤‡∏á (Concat) ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ---\n",
    "    if not all_processed_data:\n",
    "        print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏î‡πÜ ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡πÄ‡∏•‡∏¢!\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"...‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å 365+ ‡∏ä‡∏µ‡∏ï...\")\n",
    "        \n",
    "        final_rain_df_raw = pd.concat(all_processed_data, ignore_index=True)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"‚úÖ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ {end_time - start_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)\")\n",
    "        print(f\"   ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö (‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏£‡∏≠‡∏á): {final_rain_df_raw.shape[0]} ‡πÅ‡∏ñ‡∏ß\")\n",
    "\n",
    "        \n",
    "        # --- 5. [V4.3] ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà (‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£) ‡∏≠‡∏≠‡∏Å ---\n",
    "        print(\"\\n--- üóëÔ∏è ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà ‡∏Å‡∏ó‡∏°. ---\")\n",
    "        \n",
    "        final_rain_df = final_rain_df_raw[\n",
    "            final_rain_df_raw['‡πÄ‡∏Ç‡∏ï'] != '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£'\n",
    "        ].copy() \n",
    "        \n",
    "        removed_rows = len(final_rain_df_raw) - len(final_rain_df)\n",
    "        print(f\"   ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£' ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ {removed_rows} ‡πÅ‡∏ñ‡∏ß\")\n",
    "        print(f\"   ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡∏Å‡∏ó‡∏°.): {final_rain_df.shape[0]} ‡πÅ‡∏ñ‡∏ß\")\n",
    "\n",
    "\n",
    "        # --- 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡πÅ‡∏•‡∏∞ [V.7] ‡πÅ‡∏õ‡∏•‡∏á DCODE ---\n",
    "        unmatched_rows = final_rain_df[final_rain_df['dcode'].isna()]\n",
    "        unmatched_count = len(unmatched_rows)\n",
    "        \n",
    "        if unmatched_count > 0:\n",
    "            print(f\"\\n   ‚ö†Ô∏è ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏°‡∏µ {unmatched_count} ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏´‡∏≤ DCODE ‡πÑ‡∏°‡πà‡∏û‡∏ö\")\n",
    "            unmatched_districts = unmatched_rows['‡πÄ‡∏Ç‡∏ï'].unique()\n",
    "            print(f\"   ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡∏´‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö: {unmatched_districts[:10]}\")\n",
    "        else:\n",
    "            print(\"\\n   ‚úÖ DCODE ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡πÉ‡∏ô ‡∏Å‡∏ó‡∏°.) ‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!\")\n",
    "            \n",
    "            # --- üåü [‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï V.7] ‡πÅ‡∏õ‡∏•‡∏á DCODE ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏° ---\n",
    "            # ‡∏ó‡∏≥‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß null (‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£) ‡∏≠‡∏≠‡∏Å‡πÅ‡∏•‡πâ‡∏ß\n",
    "            # ‡πÅ‡∏•‡∏∞‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ null ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏•‡πâ‡∏ß\n",
    "            try:\n",
    "                final_rain_df['dcode'] = final_rain_df['dcode'].astype(int)\n",
    "                print(\"   - [V.7] ‡πÅ‡∏õ‡∏•‡∏á dcode ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏° (int) ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
    "            except ValueError as e:\n",
    "                print(f\"   - [!] ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á dcode ‡πÄ‡∏õ‡πá‡∏ô int ‡πÑ‡∏î‡πâ: {e}\")\n",
    "            \n",
    "            \n",
    "        # --- 7. ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏•‡∏ö '‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•' ---\n",
    "        \n",
    "        final_rain_df_sorted = final_rain_df # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏ß‡πâ\n",
    "        \n",
    "        if 'dcode' in final_rain_df.columns:\n",
    "            # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ\n",
    "            key_cols = ['dcode', 'dname', '‡πÄ‡∏Ç‡∏ï', '‡∏£‡∏´‡∏±‡∏™‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ', '‡∏ß‡∏±‡∏ô-‡πÄ‡∏ß‡∏•‡∏≤']\n",
    "            \n",
    "            # [‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï] ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å ('join_key' ‡πÅ‡∏•‡∏∞ '‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•')\n",
    "            other_cols = [\n",
    "                col for col in final_rain_df.columns \n",
    "                if col not in key_cols and col not in ['join_key', '‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•']\n",
    "            ]\n",
    "            \n",
    "            # ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç + ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ (‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß)\n",
    "            final_rain_df_sorted = final_rain_df[key_cols + other_cols]\n",
    "            print(\"\\n   [i] ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•' ‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "\n",
    "        # --- 8. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå ---\n",
    "        output_dir = '../data/PROCESSED/'\n",
    "        output_filename = 'rain_2024_combined_bkk_only.csv' \n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nüíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡∏Å‡∏ó‡∏°.) ‡πÑ‡∏õ‡∏ó‡∏µ‡πà: {output_path}\")\n",
    "        final_rain_df_sorted.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(\"üéâ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d059e398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ V.5 (‡∏â‡∏ö‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç - ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏ä‡∏µ‡∏ï Excel) ---\n",
      "‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ../data/PROCESSED/overall_data.xlsx\n",
      "\n",
      "üó∫Ô∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á '‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï' ‡∏à‡∏≤‡∏Å‡∏ä‡∏µ‡∏ï: 'districts'\n",
      "   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á '‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏°‡∏µ 50 ‡πÄ‡∏Ç‡∏ï‡πÅ‡∏°‡πà‡∏ö‡∏ó)\n",
      "\n",
      "==================================================\n",
      "üîß TASK 1: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° dcode ‡πÉ‡∏´‡πâ‡∏ä‡∏µ‡∏ï: 'risk'\n",
      "‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: Worksheet named 'risk' not found\n",
      "   ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏µ‡∏ï 'districts', 'risk', 'pump_readiness' ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Regular Expressions\n",
    "\n",
    "# --- 1. [V4.2] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏ï (‡∏ï‡∏±‡∏ß‡πÄ‡∏Å‡πà‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏¥‡∏°) ---\n",
    "def normalize_district_name_v4_2(name):\n",
    "    \"\"\"\n",
    "    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô V4.2:\n",
    "    1. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠/‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î (‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø, ‡∏£‡∏≤‡∏©‡∏è‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞)\n",
    "    2. ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏Ç‡∏ï, ‡∏≠‡∏≥‡πÄ‡∏†‡∏≠)\n",
    "    3. ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô (\\xa0)\n",
    "    4. ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏∏‡∏Å‡∏ä‡∏ô‡∏¥‡∏î ( \\s+ ) ‡∏ó‡∏¥‡πâ‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    \n",
    "    clean_name = str(name)\n",
    "    clean_name = clean_name.replace('‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø', '‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡πà‡∏≤‡∏¢') # ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠\n",
    "    clean_name = clean_name.replace('‡∏£‡∏≤‡∏©‡∏è‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞', '‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞')    # ‡πÅ‡∏Å‡πâ ‡∏è -> ‡∏é\n",
    "    clean_name = clean_name.replace('‡πÄ‡∏Ç‡∏ï', '')\n",
    "    clean_name = clean_name.replace('‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '') \n",
    "    clean_name = clean_name.replace(u'\\xa0', '')\n",
    "    clean_name = re.sub(r'\\s+', '', clean_name)\n",
    "    \n",
    "    return clean_name\n",
    "\n",
    "# --- 2. üåü [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÑ‡∏ü‡∏•‡πå Excel ‡∏´‡∏•‡∏±‡∏Å ---\n",
    "\n",
    "# (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥) ‡πÉ‡∏ä‡πâ Relative Path (‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å .ipynb)\n",
    "file_path = '../data/PROCESSED/overall_data.xlsx'\n",
    "\n",
    "# (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å) ‡πÉ‡∏ä‡πâ Absolute Path ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏£‡∏∞‡∏ö‡∏∏‡∏°‡∏≤\n",
    "# file_path = '/Users/tk/Desktop/flowcast-flood-prediction/data/PROCESSED/overall_data.xlsx'\n",
    "\n",
    "# üåü [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠ \"‡∏ä‡∏µ‡∏ï\" (Sheet) ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "district_sheet = 'districts'\n",
    "risk_sheet = 'risk'\n",
    "pump_sheet = 'pump_readiness'\n",
    "\n",
    "# ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå\n",
    "output_dir = '../data/PROCESSED/'\n",
    "os.makedirs(output_dir, exist_ok=True) # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ\n",
    "\n",
    "print(\"--- üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ V.5 (‡∏â‡∏ö‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç - ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏ä‡∏µ‡∏ï Excel) ---\")\n",
    "print(f\"‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: {file_path}\")\n",
    "\n",
    "try:\n",
    "    # --- 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á \"‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï\" (District Map) ---\n",
    "    print(f\"\\nüó∫Ô∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á '‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï' ‡∏à‡∏≤‡∏Å‡∏ä‡∏µ‡∏ï: '{district_sheet}'\")\n",
    "    \n",
    "    # üåü [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel ‡πÅ‡∏•‡∏∞‡∏ä‡∏µ‡∏ï‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏\n",
    "    df_districts = pd.read_excel(file_path, sheet_name=district_sheet)\n",
    "    \n",
    "    df_district_map = df_districts[['dcode', 'dname']].copy()\n",
    "    df_district_map['join_key'] = df_district_map['dname'].apply(normalize_district_name_v4_2)\n",
    "    df_district_map = df_district_map.drop_duplicates(subset=['join_key'])\n",
    "    \n",
    "    print(f\"   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á '‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏°‡∏µ {len(df_district_map)} ‡πÄ‡∏Ç‡∏ï‡πÅ‡∏°‡πà‡∏ö‡∏ó)\")\n",
    "\n",
    "    \n",
    "    # --- 4. TASK 1: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ä‡∏µ‡∏ï risk ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"üîß TASK 1: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° dcode ‡πÉ‡∏´‡πâ‡∏ä‡∏µ‡∏ï: '{risk_sheet}'\")\n",
    "    \n",
    "    # üåü [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel ‡πÅ‡∏•‡∏∞‡∏ä‡∏µ‡∏ï‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏\n",
    "    df_risk = pd.read_excel(file_path, sheet_name=risk_sheet)\n",
    "    print(f\"   - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö {risk_sheet}: {df_risk.shape[0]} ‡πÅ‡∏ñ‡∏ß\")\n",
    "    \n",
    "    df_risk['join_key'] = df_risk['‡πÄ‡∏Ç‡∏ï'].apply(normalize_district_name_v4_2)\n",
    "    \n",
    "    df_risk_processed = pd.merge(\n",
    "        df_risk.drop(columns=['join_key']), # ‡∏•‡∏ö key ‡∏ó‡∏¥‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á merge\n",
    "        df_district_map.drop(columns=['join_key']), # ‡∏•‡∏ö key ‡∏ó‡∏¥‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á merge\n",
    "        left_on=df_risk['join_key'], # ‡πÉ‡∏ä‡πâ key ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á\n",
    "        right_on=df_district_map['join_key'], # ‡πÉ‡∏ä‡πâ key ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á\n",
    "        how='left'\n",
    "    ).drop(columns=['key_0']) # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå key_0 ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ merge\n",
    "\n",
    "    # 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå\n",
    "    output_risk_path = os.path.join(output_dir, 'risk_with_dcode.csv')\n",
    "    df_risk_processed.to_csv(output_risk_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"   üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå risk ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà: {output_risk_path}\")\n",
    "\n",
    "\n",
    "    # --- 5. TASK 2: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ä‡∏µ‡∏ï pump_readiness ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"üîß TASK 2: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç dcode ‡πÉ‡∏ô‡∏ä‡∏µ‡∏ï: '{pump_sheet}'\")\n",
    "    \n",
    "    # üåü [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel ‡πÅ‡∏•‡∏∞‡∏ä‡∏µ‡∏ï‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏\n",
    "    df_pump = pd.read_excel(file_path, sheet_name=pump_sheet)\n",
    "    print(f\"   - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö {pump_sheet}: {df_pump.shape[0]} ‡πÅ‡∏ñ‡∏ß\")\n",
    "    \n",
    "    # 1. [‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç] ‡∏•‡∏ö dcode ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà \"‡∏ú‡∏¥‡∏î\" ‡∏≠‡∏≠‡∏Å\n",
    "    if 'dcode' in df_pump.columns:\n",
    "        df_pump_corrected = df_pump.drop(columns=['dcode'])\n",
    "        print(\"   - ‡∏•‡∏ö dcode ‡πÄ‡∏Å‡πà‡∏≤ (‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î) ‡∏≠‡∏≠‡∏Å‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "    else:\n",
    "        df_pump_corrected = df_pump.copy()\n",
    "        \n",
    "    # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á join_key (‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå \"dname\")\n",
    "    df_pump_corrected['join_key'] = df_pump_corrected['dname'].apply(normalize_district_name_v4_2)\n",
    "    \n",
    "    # 3. Merge ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á dcode ‡πÅ‡∏•‡∏∞ dname ‡∏ó‡∏µ‡πà \"‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\" ‡∏à‡∏≤‡∏Å \"‡πÅ‡∏°‡πà‡∏ö‡∏ó\"\n",
    "    df_pump_processed = pd.merge(\n",
    "        df_pump_corrected,\n",
    "        df_district_map, # .drop(columns=['dname']) # ‡πÄ‡∏≠‡∏≤ dname ‡∏à‡∏≤‡∏Å‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏°‡∏≤‡πÄ‡∏•‡∏¢\n",
    "        on='join_key',\n",
    "        how='left',\n",
    "        suffixes=('_old', None) # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ dname ‡∏ã‡πâ‡∏≥ ‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà (‡∏à‡∏≤‡∏Å‡πÅ‡∏°‡πà‡∏ö‡∏ó)\n",
    "    )\n",
    "\n",
    "    # 4. ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (‡πÄ‡∏≠‡∏≤ dcode, dname ‡πÉ‡∏´‡∏°‡πà‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡∏´‡∏ô‡πâ‡∏≤‡πÜ)\n",
    "    # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏≤‡∏Å df_pump ‡∏≠‡∏≠‡∏Å (‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤ 'dname_old', 'join_key')\n",
    "    original_cols = [col for col in df_pump.columns if col not in ['dcode', 'dname']]\n",
    "    \n",
    "    final_cols = ['dcode', 'dname'] + original_cols\n",
    "    final_cols_exist = [col for col in final_cols if col in df_pump_processed.columns]\n",
    "    df_pump_processed = df_pump_processed[final_cols_exist]\n",
    "\n",
    "    # 5. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå\n",
    "    output_pump_path = os.path.join(output_dir, 'pump_readiness_corrected_dcode.csv')\n",
    "    df_pump_processed.to_csv(output_pump_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"   üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå pump ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà: {output_pump_path}\")\n",
    "    \n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üéâ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ V.5 (‡∏â‡∏ö‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç) ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå\")\n",
    "    print(f\"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á: {file_path}\")\n",
    "except ValueError as e:\n",
    "    # Error ‡∏ô‡∏µ‡πâ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡∏ñ‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏µ‡∏ï (sheet_name) ‡∏ú‡∏¥‡∏î\n",
    "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
    "    print(f\"   ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏µ‡∏ï '{district_sheet}', '{risk_sheet}', '{pump_sheet}' ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\")\n",
    "except KeyError as e:\n",
    "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\")\n",
    "    print(f\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏∑‡πà‡∏≠ {e} (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô '‡πÄ‡∏Ç‡∏ï' ‡∏´‡∏£‡∏∑‡∏≠ 'dname')\")\n",
    "    print(\"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fc68311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ V.7 (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç dcode ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏°) ---\n",
      "‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ../data/PROCESSED/overall_data.xlsx\n",
      "\n",
      "üó∫Ô∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á '‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï' ‡∏à‡∏≤‡∏Å‡∏ä‡∏µ‡∏ï: 'districts'\n",
      "   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á '‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï' (V.6) ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
      "\n",
      "==================================================\n",
      "üîß TASK 1: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° dcode ‡πÉ‡∏´‡πâ‡∏ä‡∏µ‡∏ï: 'risk'\n",
      "‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: Worksheet named 'risk' not found\n",
      "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Path, ‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏µ‡∏ï, ‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Regular Expressions\n",
    "\n",
    "# --- 1. [V.6] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏ï (‡∏ï‡∏±‡∏ß‡πÄ‡∏Å‡πà‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏¥‡∏°) ---\n",
    "def normalize_V6(name):\n",
    "    \"\"\"\n",
    "    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô V.6: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Typos ('‡∏Ñ‡∏•‡∏≠‡∏á‡∏™‡∏°‡∏ß‡∏≤', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏ô‡πÄ‡∏Ç‡∏ï', '‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø', '‡∏£‡∏≤‡∏©‡∏è‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞')\n",
    "    ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤/‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    \n",
    "    clean_name = str(name)\n",
    "    \n",
    "    # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Typos/‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠\n",
    "    clean_name = clean_name.replace('‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø', '‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡πà‡∏≤‡∏¢')\n",
    "    clean_name = clean_name.replace('‡∏£‡∏≤‡∏©‡∏è‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞', '‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞')\n",
    "    clean_name = clean_name.replace('‡∏Ñ‡∏•‡∏≠‡∏á‡∏™‡∏°‡∏ß‡∏≤', '‡∏Ñ‡∏•‡∏≠‡∏á‡∏™‡∏≤‡∏°‡∏ß‡∏≤')\n",
    "    \n",
    "    # 2. ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤ (Prefixes)\n",
    "    clean_name = clean_name.replace('‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏ô‡πÄ‡∏Ç‡∏ï', '') \n",
    "    clean_name = clean_name.replace('‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÄ‡∏Ç‡∏ï', '')\n",
    "    clean_name = clean_name.replace('‡πÄ‡∏Ç‡∏ï', '')\n",
    "    clean_name = clean_name.replace('‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '') \n",
    "    \n",
    "    # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏ñ‡∏ß‡∏û‡∏¥‡πÄ‡∏®‡∏©\n",
    "    if '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏¢‡∏ô‡πâ‡∏≥' in clean_name:\n",
    "        return '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏¢‡∏ô‡πâ‡∏≥' \n",
    "    \n",
    "    # 4. ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n",
    "    clean_name = clean_name.replace(u'\\xa0', '')\n",
    "    clean_name = re.sub(r'\\s+', '', clean_name)\n",
    "    \n",
    "    return clean_name\n",
    "\n",
    "# --- 2. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ---\n",
    "file_path = '../data/PROCESSED/overall_data.xlsx'\n",
    "district_sheet = 'districts'\n",
    "risk_sheet = 'risk'\n",
    "pump_sheet = 'pump_readiness'\n",
    "\n",
    "output_dir = '../data/PROCESSED/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"--- üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ V.7 (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç dcode ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏°) ---\")\n",
    "print(f\"‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: {file_path}\")\n",
    "\n",
    "try:\n",
    "    # --- 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á \"‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï\" (District Map) ---\n",
    "    print(f\"\\nüó∫Ô∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á '‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï' ‡∏à‡∏≤‡∏Å‡∏ä‡∏µ‡∏ï: '{district_sheet}'\")\n",
    "    \n",
    "    df_districts = pd.read_excel(file_path, sheet_name=district_sheet)\n",
    "    \n",
    "    df_district_map = df_districts[['dcode', 'dname']].copy()\n",
    "    df_district_map['join_key'] = df_district_map['dname'].apply(normalize_V6)\n",
    "    df_district_map = df_district_map.drop_duplicates(subset=['join_key'])\n",
    "    \n",
    "    print(f\"   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á '‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï' (V.6) ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
    "\n",
    "    \n",
    "    # --- 4. TASK 1: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ä‡∏µ‡∏ï risk ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"üîß TASK 1: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° dcode ‡πÉ‡∏´‡πâ‡∏ä‡∏µ‡∏ï: '{risk_sheet}'\")\n",
    "    \n",
    "    df_risk = pd.read_excel(file_path, sheet_name=risk_sheet)\n",
    "    df_risk['join_key'] = df_risk['‡πÄ‡∏Ç‡∏ï'].apply(normalize_V6)\n",
    "    \n",
    "    df_risk_processed = pd.merge(\n",
    "        df_risk,\n",
    "        df_district_map,\n",
    "        on='join_key',\n",
    "        how='left',\n",
    "        suffixes=('_old', None)\n",
    "    )\n",
    "    \n",
    "    # üåü [V.7] ‡πÅ‡∏õ‡∏•‡∏á dcode ‡πÄ‡∏õ‡πá‡∏ô int (‡∏™‡∏±‡∏ô‡∏ô‡∏¥‡∏©‡∏ê‡∏≤‡∏ô‡∏ß‡πà‡∏≤ risk ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÅ‡∏ñ‡∏ß null)\n",
    "    if 'dcode' in df_risk_processed.columns and df_risk_processed['dcode'].isna().sum() == 0:\n",
    "        df_risk_processed['dcode'] = df_risk_processed['dcode'].astype(int)\n",
    "        print(\"   - [V.7] ‡πÅ‡∏õ‡∏•‡∏á dcode ‡∏Ç‡∏≠‡∏á risk ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏° (int) ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
    "    \n",
    "    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå risk\n",
    "    output_risk_path = os.path.join(output_dir, 'risk_with_dcode.csv')\n",
    "    df_risk_processed.to_csv(output_risk_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"   üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå risk ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß (V.7) ‡πÑ‡∏õ‡∏ó‡∏µ‡πà: {output_risk_path}\")\n",
    "\n",
    "\n",
    "    # --- 5. TASK 2: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ä‡∏µ‡∏ï pump_readiness ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"üîß TASK 2: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç dcode ‡πÉ‡∏ô‡∏ä‡∏µ‡∏ï: '{pump_sheet}'\")\n",
    "    \n",
    "    df_pump = pd.read_excel(file_path, sheet_name=pump_sheet)\n",
    "    \n",
    "    df_pump_corrected = df_pump.drop(columns=['dcode'], errors='ignore')\n",
    "    print(f\"   - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö {pump_sheet}: {df_pump.shape[0]} ‡πÅ‡∏ñ‡∏ß (‡∏•‡∏ö dcode ‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏•‡πâ‡∏ß)\")\n",
    "        \n",
    "    df_pump_corrected['join_key'] = df_pump_corrected['dname'].apply(normalize_V6)\n",
    "    \n",
    "    df_pump_processed = pd.merge(\n",
    "        df_pump_corrected,\n",
    "        df_district_map,\n",
    "        on='join_key',\n",
    "        how='left',\n",
    "        suffixes=('_old', None)\n",
    "    )\n",
    "    \n",
    "    # 1. ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß null (‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏¢‡∏ô‡πâ‡∏≥)\n",
    "    df_pump_final = df_pump_processed.dropna(subset=['dcode']).copy()\n",
    "    removed_count = len(df_pump_processed) - len(df_pump_final)\n",
    "    print(f\"   - ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà dcode ‡πÄ‡∏õ‡πá‡∏ô null ‡∏≠‡∏≠‡∏Å {removed_count} ‡πÅ‡∏ñ‡∏ß\")\n",
    "\n",
    "    # 2. üåü [V.7] ‡πÅ‡∏õ‡∏•‡∏á dcode ‡πÄ‡∏õ‡πá‡∏ô int (‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏•‡∏ö null ‡πÅ‡∏•‡πâ‡∏ß)\n",
    "    df_pump_final['dcode'] = df_pump_final['dcode'].astype(int)\n",
    "    print(\"   - [V.7] ‡πÅ‡∏õ‡∏•‡∏á dcode ‡∏Ç‡∏≠‡∏á pump ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏° (int) ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
    "\n",
    "    # 3. ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
    "    original_cols = [col for col in df_pump.columns if col not in ['dcode', 'dname']]\n",
    "    final_cols = ['dcode', 'dname'] + original_cols\n",
    "    final_cols_exist = [col for col in final_cols if col in df_pump_final.columns]\n",
    "    df_pump_processed_sorted = df_pump_final[final_cols_exist] \n",
    "\n",
    "    # 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå\n",
    "    output_pump_path = os.path.join(output_dir, 'pump_readiness_corrected_dcode.csv')\n",
    "    df_pump_processed_sorted.to_csv(output_pump_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"   üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå pump ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß (V.7) ‡πÑ‡∏õ‡∏ó‡∏µ‡πà: {output_pump_path}\")\n",
    "    \n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üéâ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ V.7 ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
    "    print(\"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Path, ‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏µ‡∏ï, ‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5808703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: ../data/PROCESSED/rain_2024_combined_bkk_only.csv ---\n",
      "   ... ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à 18300 ‡πÅ‡∏ñ‡∏ß\n",
      "--- 2. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡∏û.‡∏®. 2567 ‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ.‡∏®. 2024 ... ---\n",
      "   ... (‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô) ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ 11100 ‡πÅ‡∏ñ‡∏ß\n",
      "   ... ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô datetime object (‡∏Ñ.‡∏®.) ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\n",
      "--- 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'season' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ---\n",
      "              ‡∏ß‡∏±‡∏ô-‡πÄ‡∏ß‡∏•‡∏≤ date_object  season\n",
      "0  2567-01-01 00:00:00  2024-01-01  Winter\n",
      "1  2567-01-01 00:00:00  2024-01-01  Winter\n",
      "2  2567-01-01 00:00:00  2024-01-01  Winter\n",
      "3  2567-01-01 00:00:00  2024-01-01  Winter\n",
      "4  2567-01-01 00:00:00  2024-01-01  Winter\n",
      "--- 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡∏ó‡∏µ‡πà ../data/PROCESSED/rain_2024_with_seasons.csv ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢! ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. ‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ö‡πà‡∏á‡∏§‡∏î‡∏π ---\n",
    "# (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏±‡∏ö Date object (‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ.‡∏®. ‡πÅ‡∏•‡πâ‡∏ß) ‡πÄ‡∏õ‡πá‡∏ô input)\n",
    "def get_season(date):\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    \n",
    "    # ‡∏§‡∏î‡∏π‡∏£‡πâ‡∏≠‡∏ô (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 16 ‡∏Å.‡∏û. - 15 ‡∏û.‡∏Ñ.)\n",
    "    if (month == 2 and day >= 16) or (month in [3, 4]) or (month == 5 and day <= 15):\n",
    "        return 'Summer'\n",
    "    # ‡∏§‡∏î‡∏π‡∏ù‡∏ô (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 16 ‡∏û.‡∏Ñ. - 15 ‡∏ï.‡∏Ñ.)\n",
    "    elif (month == 5 and day >= 16) or (month in [6, 7, 8, 9]) or (month == 10 and day <= 15):\n",
    "        return 'Rainy'\n",
    "    # ‡∏§‡∏î‡∏π‡∏´‡∏ô‡∏≤‡∏ß (‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠)\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "# --- 2. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ù‡∏ô‡πÄ‡∏î‡∏¥‡∏° ---\n",
    "# (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!) ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Path ‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏à‡∏≤‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á Notebook ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
    "rain_file_path = '../data/PROCESSED/rain_2024_combined_bkk_only.csv' \n",
    "print(f\"--- 1. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: {rain_file_path} ---\")\n",
    "\n",
    "try:\n",
    "    rain_df = pd.read_csv(rain_file_path)\n",
    "    print(f\"   ... ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {len(rain_df)} ‡πÅ‡∏ñ‡∏ß\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"!!! Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {rain_file_path}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 3. [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡∏°‡πà] ‡∏ó‡∏≥ Feature Engineering (‡∏ã‡πà‡∏≠‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡∏û.‡∏®.) ---\n",
    "print(\"--- 2. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡∏û.‡∏®. 2567 ‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ.‡∏®. 2024 ... ---\")\n",
    "try:\n",
    "    # 1. ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡∏ß‡∏±‡∏ô-‡πÄ‡∏ß‡∏•‡∏≤' ‡πÄ‡∏õ‡πá‡∏ô string (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ .str ‡πÑ‡∏î‡πâ)\n",
    "    date_str = rain_df['‡∏ß‡∏±‡∏ô-‡πÄ‡∏ß‡∏•‡∏≤'].astype(str)\n",
    "    \n",
    "    # 2. (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç) ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà '2567' (‡∏û.‡∏®.) ‡∏î‡πâ‡∏ß‡∏¢ '2024' (‡∏Ñ.‡∏®.)\n",
    "    corrected_date_str = date_str.str.replace('2567', '2024')\n",
    "    \n",
    "    # 3. (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç) ‡πÅ‡∏õ‡∏•‡∏á String ‡∏Ñ.‡∏®. ‡πÉ‡∏´‡∏°‡πà ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô datetime object\n",
    "    #    - dayfirst=True: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡∏≠‡πà‡∏≤‡∏ô '29/01/2024' (DD/MM/YYYY) ‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
    "    #    - errors='coerce': ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏á‡∏à‡∏£‡∏¥‡∏á‡πÜ (‡πÄ‡∏ä‡πà‡∏ô 'nan', '29/0') ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô NaT (Not a Time)\n",
    "    rain_df['date_object'] = pd.to_datetime(corrected_date_str, dayfirst=True, errors='coerce')\n",
    "    \n",
    "    # 4. (‡πÉ‡∏´‡∏°‡πà) ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏á (NaT)\n",
    "    original_rows = len(rain_df)\n",
    "    rain_df.dropna(subset=['date_object'], inplace=True)\n",
    "    dropped_rows = original_rows - len(rain_df)\n",
    "    \n",
    "    if dropped_rows > 0:\n",
    "        print(f\"   ... (‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô) ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ {dropped_rows} ‡πÅ‡∏ñ‡∏ß\")\n",
    "    print(\"   ... ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô datetime object (‡∏Ñ.‡∏®.) ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"!!! Error ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'season' ---\n",
    "rain_df['season'] = rain_df['date_object'].apply(get_season)\n",
    "print(\"--- 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'season' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ---\")\n",
    "print(rain_df[['‡∏ß‡∏±‡∏ô-‡πÄ‡∏ß‡∏•‡∏≤', 'date_object', 'season']].head())\n",
    "\n",
    "# --- 5. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà ---\n",
    "output_path = '../data/PROCESSED/rain_2024_with_seasons.csv'\n",
    "rain_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"--- 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡∏ó‡∏µ‡πà {output_path} ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7fe904",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2851573667.py, line 71)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m```eof\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"--- Correlation Matrix Export Script Started ---\")\n",
    "\n",
    "# --- 1. Define Paths ---\n",
    "# Path to the source data (assuming notebook is in 'notebooks' folder)\n",
    "input_csv_path = '../data/PROCESSED/master_district_features.csv'\n",
    "\n",
    "# Path to the frontend public data directory\n",
    "output_json_dir = '../frontend/public/data'\n",
    "output_json_path = os.path.join(output_json_dir, 'correlation_matrix.json')\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_json_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Loading data from: {input_csv_path}\")\n",
    "\n",
    "try:\n",
    "    # --- 2. Load Data ---\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # --- 3. Select Features ---\n",
    "    features_to_correlate = [\n",
    "        'pump_capacity_total', \n",
    "        'canal_count', \n",
    "        'num_risk_points', \n",
    "        'AREA'\n",
    "    ]\n",
    "    \n",
    "    # Check if all columns exist\n",
    "    missing_cols = [col for col in features_to_correlate if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Error: Missing columns in the CSV: {missing_cols}\")\n",
    "    else:\n",
    "        df_features = df[features_to_correlate]\n",
    "        \n",
    "        # --- 4. Calculate Correlation ---\n",
    "        # Using Pearson correlation\n",
    "        corr_matrix = df_features.corr(method='pearson')\n",
    "        \n",
    "        print(\"\\n--- Calculated Correlation Matrix ---\")\n",
    "        print(corr_matrix)\n",
    "        \n",
    "        # --- 5. Export to JSON ---\n",
    "        # Use orient='index' for a clean key-value structure: {row: {col: value}}\n",
    "        # This format is very easy for JavaScript/React to read\n",
    "        corr_matrix.to_json(output_json_path, orient='index', indent=4)\n",
    "        \n",
    "        print(f\"\\n[SUCCESS] Successfully exported correlation matrix to:\")\n",
    "        print(output_json_path)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at {input_csv_path}\")\n",
    "    print(\"Please check the path and run the script again.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "print(\"--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c9a438e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Actual Columns in master_district_features.csv ---\n",
      "['OBJECTID', 'AREA', 'dcode', 'dname', 'dname_e', 'pcode', 'pname', 'num_male', 'num_female', 'num_school', 'num_hos', 'num_comm', 'num_temple', 'num_health', 'geometry', 'population', 'canal_count', 'flood_point_count', 'floodgate_count', 'avg_rain_rainy', 'avg_rain_summer', 'avg_rain_winter', 'district_group', 'budget_year', 'inspec_period', 'pump_number', 'pump_ready', 'pump_repair', 'pump_sell', 'pump_support', 'total_ready', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏∏‡∏î‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏∏‡∏î‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ô‡πâ‡∏≥‡∏ó‡πà‡∏ß‡∏°']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None) # ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
    "df_check = pd.read_csv('../data/PROCESSED/master_district_features.csv')\n",
    "print(\"--- Actual Columns in master_district_features.csv ---\")\n",
    "print(list(df_check.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9d2fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Correlation Matrix Export Script (V2 - Updated) ---\n",
      "Loading data from: ../data/PROCESSED/master_district_features.csv\n",
      "\n",
      "[ERROR] Still missing columns: ['YOUR_CORRECT_PUMP_COLUMN_NAME', 'YOUR_CORRECT_RISK_COLUMN_NAME']\n",
      "Please check your spelling in the 'features_to_correlate' list.\n",
      "\n",
      "Available columns are:\n",
      "['OBJECTID', 'AREA', 'dcode', 'dname', 'dname_e', 'pcode', 'pname', 'num_male', 'num_female', 'num_school', 'num_hos', 'num_comm', 'num_temple', 'num_health', 'geometry', 'population', 'canal_count', 'flood_point_count', 'floodgate_count', 'avg_rain_rainy', 'avg_rain_summer', 'avg_rain_winter', 'district_group', 'budget_year', 'inspec_period', 'pump_number', 'pump_ready', 'pump_repair', 'pump_sell', 'pump_support', 'total_ready', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏∏‡∏î‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏∏‡∏î‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ô‡πâ‡∏≥‡∏ó‡πà‡∏ß‡∏°']\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"--- Correlation Matrix Export Script (V2 - Updated) ---\")\n",
    "\n",
    "# --- [!!!] ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏ô‡∏µ‡πâ [!!!] ---\n",
    "#\n",
    "# ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'pump_capacity_total' ‡πÅ‡∏•‡∏∞ 'num_risk_points' ‡∏à‡∏≤‡∏Å‡πÅ‡∏ú‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ \"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á\" ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV\n",
    "#\n",
    "# 1. ‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î (print(list(df_check.columns))) ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1\n",
    "# 2. ‡∏ô‡∏≥‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå 'features_to_correlate' ‡∏ô‡∏µ‡πâ\n",
    "#\n",
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ñ‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏∑‡∏≠ 'pump_total' ‡πÅ‡∏•‡∏∞ 'num_risk'\n",
    "# features_to_correlate = [\n",
    "#     'pump_total', \n",
    "#     'canal_count', \n",
    "#     'num_risk', \n",
    "#     'AREA'\n",
    "# ]\n",
    "# -----------------------------------------------------------------\n",
    "features_to_correlate = [\n",
    "    'YOUR_CORRECT_PUMP_COLUMN_NAME',  # <-- FIXME: ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå \"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏π‡∏ö\" ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
    "    'canal_count',                     # (‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏•‡πâ‡∏ß)\n",
    "    'YOUR_CORRECT_RISK_COLUMN_NAME', # <-- FIXME: ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå \"‡∏à‡∏∏‡∏î‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á\" ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
    "    'AREA'                           # (‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏•‡πâ‡∏ß)\n",
    "]\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# --- 1. Define Paths ---\n",
    "input_csv_path = '../data/PROCESSED/master_district_features.csv'\n",
    "output_json_dir = '../frontend/public/data'\n",
    "output_json_path = os.path.join(output_json_dir, 'correlation_matrix.json')\n",
    "\n",
    "os.makedirs(output_json_dir, exist_ok=True)\n",
    "print(f\"Loading data from: {input_csv_path}\")\n",
    "\n",
    "try:\n",
    "    # --- 2. Load Data ---\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # --- 3. Check Features ---\n",
    "    missing_cols = [col for col in features_to_correlate if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\n[ERROR] Still missing columns: {missing_cols}\")\n",
    "        print(\"Please check your spelling in the 'features_to_correlate' list.\")\n",
    "        print(\"\\nAvailable columns are:\")\n",
    "        print(list(df.columns))\n",
    "    else:\n",
    "        df_features = df[features_to_correlate]\n",
    "        \n",
    "        # --- 4. Calculate Correlation ---\n",
    "        corr_matrix = df_features.corr(method='pearson')\n",
    "        \n",
    "        print(\"\\n--- Calculated Correlation Matrix ---\")\n",
    "        print(corr_matrix)\n",
    "        \n",
    "        # --- 5. Export to JSON ---\n",
    "        corr_matrix.to_json(output_json_path, orient='index', indent=4)\n",
    "        \n",
    "        print(f\"\\n[SUCCESS] Successfully exported correlation matrix to:\")\n",
    "        print(output_json_path)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at {input_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "print(\"--- Script Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129bb7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏: ../data/RAW/stormtrack2024.xlsx\n",
      "‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô: ../data/PROCESSED/rain_2024_with_seasons.csv\n",
      "‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà: ../data/PROCESSED/rain_with_storm_summary_2024.csv\n",
      "\n",
      "‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
      "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô: 7200 ‡πÅ‡∏ñ‡∏ß\n",
      "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏: 1683 ‡πÅ‡∏ñ‡∏ß\n",
      "\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô...\n",
      "‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 163 ‡πÅ‡∏ñ‡∏ß (‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô)\n",
      "\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏...\n",
      "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏µ: 7200 ‡πÅ‡∏ñ‡∏ß\n",
      "\n",
      "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏ó‡∏µ‡πà: ../data/PROCESSED/rain_with_storm_summary_2024.csv\n",
      "\n",
      "--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß ---\n",
      "   dcode         dname        ‡πÄ‡∏Ç‡∏ï  ‡∏£‡∏´‡∏±‡∏™‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ                        ‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ  \\\n",
      "0   1001     ‡πÄ‡∏Ç‡∏ï‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£     ‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£  RF.PNK.02                    ‡∏™.‡πÄ‡∏ó‡πÄ‡∏ß‡∏®‡∏£‡πå   \n",
      "1   1033    ‡πÄ‡∏Ç‡∏ï‡∏Ñ‡∏•‡∏≠‡∏á‡πÄ‡∏ï‡∏¢    ‡∏Ñ‡∏•‡∏≠‡∏á‡πÄ‡∏ï‡∏¢  RF.KTY.01                  ‡∏™‡∏ô‡∏Ç.‡∏Ñ‡∏•‡∏≠‡∏á‡πÄ‡∏ï‡∏¢   \n",
      "2   1018    ‡πÄ‡∏Ç‡∏ï‡∏Ñ‡∏•‡∏≠‡∏á‡∏™‡∏≤‡∏ô    ‡∏Ñ‡∏•‡∏≠‡∏á‡∏™‡∏≤‡∏ô  RF.KSN.01                  ‡∏™‡∏ô‡∏Ç.‡∏Ñ‡∏•‡∏≠‡∏á‡∏™‡∏≤‡∏ô   \n",
      "3   1046  ‡πÄ‡∏Ç‡∏ï‡∏Ñ‡∏•‡∏≠‡∏á‡∏™‡∏≤‡∏°‡∏ß‡∏≤  ‡∏Ñ‡∏•‡∏≠‡∏á‡∏™‡∏≤‡∏°‡∏ß‡∏≤  RF.KSW.01                ‡∏™‡∏ô‡∏Ç.‡∏Ñ‡∏•‡∏≠‡∏á‡∏™‡∏≤‡∏°‡∏ß‡∏≤   \n",
      "4   1043   ‡πÄ‡∏Ç‡∏ï‡∏Ñ‡∏±‡∏ô‡∏ô‡∏≤‡∏¢‡∏≤‡∏ß   ‡∏Ñ‡∏±‡∏ô‡∏ô‡∏≤‡∏¢‡∏≤‡∏ß  RF.KNY.02  ‡∏à‡∏∏‡∏î‡∏ß‡∏±‡∏î ‡∏Ñ.‡∏•‡∏≥‡∏ä‡∏∞‡∏•‡πà‡∏≤-‡∏ñ.‡∏ô‡∏ß‡∏°‡∏¥‡∏ô‡∏ó‡∏£‡πå   \n",
      "\n",
      "              ‡∏ß‡∏±‡∏ô-‡πÄ‡∏ß‡∏•‡∏≤  ‡∏ù‡∏ô 5 ‡∏ô‡∏≤‡∏ó‡∏µ  ‡∏ù‡∏ô 15 ‡∏ô‡∏≤‡∏ó‡∏µ  ‡∏ù‡∏ô 30 ‡∏ô‡∏≤‡∏ó‡∏µ  ‡∏ù‡∏ô 1 ‡∏ä‡∏°.  ‡∏ù‡∏ô 3 ‡∏ä‡∏°.  \\\n",
      "0  2567-01-01 00:00:00        0.0         0.0         0.0       0.0       7.5   \n",
      "1  2567-01-01 00:00:00        0.0         0.0         0.0       0.0       0.0   \n",
      "2  2567-01-01 00:00:00        0.0         0.0         0.0       0.0       0.0   \n",
      "3  2567-01-01 00:00:00        0.0         0.0         0.0       0.0       0.0   \n",
      "4  2567-01-01 00:00:00        0.0         0.0         0.0       0.0       0.0   \n",
      "\n",
      "   ‡∏ù‡∏ô 6 ‡∏ä‡∏°.  ‡∏ù‡∏ô 12 ‡∏ä‡∏°.  ‡∏ù‡∏ô 24 ‡∏ä‡∏°. date_object  season  active_storms_count  \\\n",
      "0       7.5        7.5        8.0  2024-01-01  Winter                    0   \n",
      "1       0.0        0.0        0.0  2024-01-01  Winter                    0   \n",
      "2       0.0        0.0        0.0  2024-01-01  Winter                    0   \n",
      "3       0.0        0.0        0.0  2024-01-01  Winter                    0   \n",
      "4       0.0        0.0        0.0  2024-01-01  Winter                    0   \n",
      "\n",
      "   max_vmax_all_storms  min_mlsp_all_storms active_storm_names  \n",
      "0                  0.0                  NaN                 []  \n",
      "1                  0.0                  NaN                 []  \n",
      "2                  0.0                  NaN                 []  \n",
      "3                  0.0                  NaN                 []  \n",
      "4                  0.0                  NaN                 []  \n",
      "\n",
      "--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß ---\n",
      "      dcode        dname       ‡πÄ‡∏Ç‡∏ï  ‡∏£‡∏´‡∏±‡∏™‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ         ‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ  \\\n",
      "7195   1042    ‡πÄ‡∏Ç‡∏ï‡∏™‡∏≤‡∏¢‡πÑ‡∏´‡∏°    ‡∏™‡∏≤‡∏¢‡πÑ‡∏´‡∏°  RF.SMI.01    ‡∏™‡∏ô‡∏Ç.‡∏™‡∏≤‡∏¢‡πÑ‡∏´‡∏°   \n",
      "7196   1023   ‡πÄ‡∏Ç‡∏ï‡∏´‡∏ô‡∏≠‡∏á‡πÅ‡∏Ç‡∏°   ‡∏´‡∏ô‡∏≠‡∏á‡πÅ‡∏Ç‡∏°  RF.NKM.01   ‡∏™‡∏ô‡∏Ç.‡∏´‡∏ô‡∏≠‡∏á‡πÅ‡∏Ç‡∏°   \n",
      "7197   1003   ‡πÄ‡∏Ç‡∏ï‡∏´‡∏ô‡∏≠‡∏á‡∏à‡∏≠‡∏Å   ‡∏´‡∏ô‡∏≠‡∏á‡∏à‡∏≠‡∏Å  RF.NJK.01   ‡∏™‡∏ô‡∏Ç.‡∏´‡∏ô‡∏≠‡∏á‡∏à‡∏≠‡∏Å   \n",
      "7198   1041   ‡πÄ‡∏Ç‡∏ï‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏µ‡πà   ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏µ‡πà  RF.LSI.01   ‡∏™‡∏ô‡∏Ç.‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏µ‡πà   \n",
      "7199   1017  ‡πÄ‡∏Ç‡∏ï‡∏´‡πâ‡∏ß‡∏¢‡∏Ç‡∏ß‡∏≤‡∏á  ‡∏´‡πâ‡∏ß‡∏¢‡∏Ç‡∏ß‡∏≤‡∏á  RF.HKG.01  ‡∏™‡∏ô‡∏Ç.‡∏´‡πâ‡∏ß‡∏¢‡∏Ç‡∏ß‡∏≤‡∏á   \n",
      "\n",
      "                 ‡∏ß‡∏±‡∏ô-‡πÄ‡∏ß‡∏•‡∏≤  ‡∏ù‡∏ô 5 ‡∏ô‡∏≤‡∏ó‡∏µ  ‡∏ù‡∏ô 15 ‡∏ô‡∏≤‡∏ó‡∏µ  ‡∏ù‡∏ô 30 ‡∏ô‡∏≤‡∏ó‡∏µ  ‡∏ù‡∏ô 1 ‡∏ä‡∏°.  \\\n",
      "7195  2567-12-12 00:00:00        0.0         0.0         0.0       0.0   \n",
      "7196  2567-12-12 00:00:00        0.0         0.0         0.0       0.0   \n",
      "7197  2567-12-12 00:00:00        0.0         0.0         0.0       0.0   \n",
      "7198  2567-12-12 00:00:00        0.0         0.0         0.0       0.0   \n",
      "7199  2567-12-12 00:00:00        0.0         0.0         0.0       0.0   \n",
      "\n",
      "      ‡∏ù‡∏ô 3 ‡∏ä‡∏°.  ‡∏ù‡∏ô 6 ‡∏ä‡∏°.  ‡∏ù‡∏ô 12 ‡∏ä‡∏°.  ‡∏ù‡∏ô 24 ‡∏ä‡∏°. date_object  season  \\\n",
      "7195       0.0       0.0        0.0        0.0  2024-12-12  Winter   \n",
      "7196       0.0       0.0        0.0        0.0  2024-12-12  Winter   \n",
      "7197       0.0       0.0        0.0        0.0  2024-12-12  Winter   \n",
      "7198       0.0       0.0        0.0        0.0  2024-12-12  Winter   \n",
      "7199       0.0       0.0        0.0        0.0  2024-12-12  Winter   \n",
      "\n",
      "      active_storms_count  max_vmax_all_storms  min_mlsp_all_storms  \\\n",
      "7195                    0                  0.0                  NaN   \n",
      "7196                    0                  0.0                  NaN   \n",
      "7197                    0                  0.0                  NaN   \n",
      "7198                    0                  0.0                  NaN   \n",
      "7199                    0                  0.0                  NaN   \n",
      "\n",
      "     active_storm_names  \n",
      "7195                 []  \n",
      "7196                 []  \n",
      "7197                 []  \n",
      "7198                 []  \n",
      "7199                 []  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå ---\n",
    "\n",
    "# ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö (‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏Å‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå)\n",
    "# ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ Notebook ‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå notebooks/\n",
    "base_path = '../'\n",
    "\n",
    "# --- [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô .xlsx ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏à‡πâ‡∏á ---\n",
    "storm_file_path = os.path.join(base_path, 'data', 'RAW', 'stormtrack2024.xlsx')\n",
    "rain_file_path = os.path.join(base_path, 'data', 'PROCESSED', 'rain_2024_with_seasons.csv')\n",
    "\n",
    "# ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å\n",
    "output_dir = os.path.join(base_path, 'data', 'PROCESSED')\n",
    "output_filename = 'rain_with_storm_summary_2024.csv'\n",
    "output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå PROCESSED ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏: {storm_file_path}\")\n",
    "print(f\"‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô: {rain_file_path}\")\n",
    "print(f\"‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà: {output_path}\")\n",
    "\n",
    "# --- 2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\n",
    "\n",
    "try:\n",
    "    # --- [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡πÉ‡∏ä‡πâ pd.read_excel ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå .xlsx ---\n",
    "    storm_df = pd.read_excel(storm_file_path)\n",
    "    \n",
    "    rain_df = pd.read_csv(rain_file_path)\n",
    "    print(\"\\n‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
    "    print(f\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô: {rain_df.shape[0]} ‡πÅ‡∏ñ‡∏ß\")\n",
    "    print(f\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏: {storm_df.shape[0]} ‡πÅ‡∏ñ‡∏ß\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n[‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {e.filename}\")\n",
    "    print(\"‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå (file path) ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\")\n",
    "    # ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏´‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    print(f\"\\n[‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î] ‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {e}\")\n",
    "    print(\"‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel (.xlsx) ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á 'openpyxl' ‡∏´‡∏£‡∏∑‡∏≠ 'xlrd' ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á (‡∏•‡∏≠‡∏á pip install openpyxl)\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "# --- 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏ (Storm Data) ---\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'yyyymmddhh' ‡πÄ‡∏õ‡πá‡∏ô string ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÑ‡∏î‡πâ\n",
    "# (‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà Excel ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç)\n",
    "storm_df['yyyymmddhh'] = storm_df['yyyymmddhh'].astype(str)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'date_object' ‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡∏î‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà (YYYYMMDD)\n",
    "storm_df['date_str'] = storm_df['yyyymmddhh'].str.slice(0, 8)\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô datetime object ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö 'YYYY-MM-DD'\n",
    "# ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô\n",
    "storm_df['date_object'] = pd.to_datetime(storm_df['date_str'], format='%Y%m%d').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# ‡∏•‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤ stormname ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 'INVEST'\n",
    "storm_df['stormname'] = storm_df['stormname'].str.strip()\n",
    "storm_df['stormname'] = storm_df['stormname'].replace('', 'INVEST') \n",
    "\n",
    "# --- 4. ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô (Aggregate Storm Data) ---\n",
    "\n",
    "print(\"\\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô...\")\n",
    "storm_summary_df = storm_df.groupby('date_object').agg(\n",
    "    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏û‡∏≤‡∏¢‡∏∏‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô\n",
    "    active_storms_count=('stom_no', 'nunique'),\n",
    "    \n",
    "    # ‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏•‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (vmax) ‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô\n",
    "    max_vmax_all_storms=('vmax', 'max'),\n",
    "    \n",
    "    # ‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏î‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î (mlsp) ‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô\n",
    "    min_mlsp_all_storms=('mlsp', 'min'),\n",
    "    \n",
    "    # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏û‡∏≤‡∏¢‡∏∏‡∏ó‡∏µ‡πà active ‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô (‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥)\n",
    "    active_storm_names=('stormname', lambda x: list(x.unique()))\n",
    ").reset_index()\n",
    "\n",
    "print(f\"‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {storm_summary_df.shape[0]} ‡πÅ‡∏ñ‡∏ß (‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô)\")\n",
    "\n",
    "# --- 5. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô (Rain Data) ---\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ 'date_object' ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô‡πÄ‡∏õ‡πá‡∏ô string 'YYYY-MM-DD'\n",
    "rain_df['date_object'] = pd.to_datetime(rain_df['date_object']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# --- 6. ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Merge Data) ---\n",
    "\n",
    "print(\"\\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏...\")\n",
    "merged_df = pd.merge(\n",
    "    rain_df,\n",
    "    storm_summary_df,\n",
    "    on='date_object',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- 7. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏π‡πà (Fill Missing Values) ---\n",
    "\n",
    "merged_df['active_storms_count'] = merged_df['active_storms_count'].fillna(0).astype(int)\n",
    "merged_df['max_vmax_all_storms'] = merged_df['max_vmax_all_storms'].fillna(0)\n",
    "merged_df['min_mlsp_all_storms'] = merged_df['min_mlsp_all_storms'].fillna(np.nan) \n",
    "merged_df['active_storm_names'] = merged_df['active_storm_names'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "print(f\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏µ: {merged_df.shape[0]} ‡πÅ‡∏ñ‡∏ß\")\n",
    "\n",
    "# --- 8. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå ---\n",
    "\n",
    "try:\n",
    "    merged_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏ó‡∏µ‡πà: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î] ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ: {e}\")\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "print(\"\\n--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß ---\")\n",
    "print(merged_df.head())\n",
    "\n",
    "print(\"\\n--- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß ---\")\n",
    "print(merged_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a288d4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üöÄ [‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1] ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô 2023 ---\n",
      "   üó∫Ô∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà DCODE ...\n",
      "   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà DCODE ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏°‡∏µ 50 ‡πÄ‡∏Ç‡∏ï)\n",
      "\n",
      "   üå¶Ô∏è ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô 2023 ...\n",
      "   üìÇ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô 12 ‡πÑ‡∏ü‡∏•‡πå. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏ô‡∏•‡∏π‡∏õ...\n",
      "      - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 01.xlsx...\n",
      "      - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 02.xlsx...\n",
      "      - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 03.xlsx...\n",
      "      - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 04.xlsx...\n",
      "      - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 05.xlsx...\n",
      "      - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 06.xlsx...\n",
      "      - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 07.xlsx...\n",
      "      - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 08.xlsx...\n",
      "      - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 09.xlsx...\n",
      "      - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 10.xlsx...\n",
      "      - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 11.xlsx...\n",
      "      - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: 12.xlsx...\n",
      "      ... ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏±‡πâ‡∏á 12 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "   ...‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å 365+ ‡∏ä‡∏µ‡∏ï...\n",
      "   ‚úÖ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 1.71 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)\n",
      "      ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö (‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏£‡∏≠‡∏á): 18309 ‡πÅ‡∏ñ‡∏ß\n",
      "\n",
      "   üóëÔ∏è ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà ‡∏Å‡∏ó‡∏°. ...\n",
      "   ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£' ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ 359 ‡πÅ‡∏ñ‡∏ß\n",
      "      ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡∏Å‡∏ó‡∏°.): 17950 ‡πÅ‡∏ñ‡∏ß\n",
      "   ‚úÖ DCODE ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡πÉ‡∏ô ‡∏Å‡∏ó‡∏°.) ‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!\n",
      "   ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á dcode ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏° (int) ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
      "   [i] ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•' ‡πÅ‡∏•‡πâ‡∏ß\n",
      "\n",
      "   üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1 ‡πÑ‡∏õ‡∏ó‡∏µ‡πà: ../data/PROCESSED/rain_2023_combined_bkk_only.csv\n",
      "   üéâ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\n",
      "\n",
      "--- üöÄ [‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2] ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏§‡∏î‡∏π 2023 ---\n",
      "   - 1. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: ../data/PROCESSED/rain_2023_combined_bkk_only.csv\n",
      "      ... ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à 17950 ‡πÅ‡∏ñ‡∏ß\n",
      "   - 2. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡∏û.‡∏®. 2566 ‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ.‡∏®. 2023 ...\n",
      "      ... ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô datetime object (‡∏Ñ.‡∏®.) ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\n",
      "   - 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'season' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
      "   - 4. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡∏ó‡∏µ‡πà ../data/PROCESSED/rain_2023_with_seasons.csv ...\n",
      "   üéâ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2 ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\n",
      "\n",
      "--- üöÄ [‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3] ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏ 2023 ---\n",
      "   - ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏: ../data/RAW/stormtrack2023.xlsx\n",
      "   - ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô: ../data/PROCESSED/rain_2023_with_seasons.csv\n",
      "\n",
      "   ... ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
      "      ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô (2023): 17950 ‡πÅ‡∏ñ‡∏ß\n",
      "      ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏ (2023): 1652 ‡πÅ‡∏ñ‡∏ß\n",
      "\n",
      "   ... ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô...\n",
      "   ... ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 188 ‡πÅ‡∏ñ‡∏ß (‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô)\n",
      "\n",
      "   ... ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏...\n",
      "   ... ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏µ: 17950 ‡πÅ‡∏ñ‡∏ß\n",
      "\n",
      "   üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏ó‡∏µ‡πà: ../data/PROCESSED/rain_with_storm_summary_2023.csv\n",
      "\n",
      "üéâüéâüéâ [‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô] ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏µ 2023 ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! üéâüéâüéâ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. [V4.2] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏ï (‡∏à‡∏≤‡∏Å Cell 5) ---\n",
    "def normalize_district_name_v4_2(name):\n",
    "    \"\"\"\n",
    "    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô V4.2:\n",
    "    1. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠/‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î (‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø, ‡∏£‡∏≤‡∏©‡∏è‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞)\n",
    "    2. ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏Ç‡∏ï, ‡∏≠‡∏≥‡πÄ‡∏†‡∏≠)\n",
    "    3. ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô (\\\\xa0)\n",
    "    4. ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏∏‡∏Å‡∏ä‡∏ô‡∏¥‡∏î ( \\\\s+ ) ‡∏ó‡∏¥‡πâ‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    \n",
    "    clean_name = str(name)\n",
    "    clean_name = clean_name.replace('‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏Ø', '‡∏õ‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏≤‡∏ö‡∏®‡∏±‡∏ï‡∏£‡∏π‡∏û‡πà‡∏≤‡∏¢') # ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠\n",
    "    clean_name = clean_name.replace('‡∏£‡∏≤‡∏©‡∏è‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞', '‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ö‡∏π‡∏£‡∏ì‡∏∞')    # ‡πÅ‡∏Å‡πâ ‡∏è -> ‡∏é\n",
    "    clean_name = clean_name.replace('‡πÄ‡∏Ç‡∏ï', '')\n",
    "    clean_name = clean_name.replace('‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', '') # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å ‡∏Å‡∏ó‡∏°.\n",
    "    clean_name = clean_name.replace(u'\\xa0', '')\n",
    "    clean_name = re.sub(r'\\s+', '', clean_name)\n",
    "    \n",
    "    return clean_name\n",
    "\n",
    "# --- 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏ö‡πà‡∏á‡∏§‡∏î‡∏π (‡∏à‡∏≤‡∏Å Cell 11) ---\n",
    "def get_season(date):\n",
    "    \"\"\"\n",
    "    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏±‡∏ö Date object (‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ.‡∏®. ‡πÅ‡∏•‡πâ‡∏ß) ‡πÄ‡∏õ‡πá‡∏ô input\n",
    "    \"\"\"\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    \n",
    "    # ‡∏§‡∏î‡∏π‡∏£‡πâ‡∏≠‡∏ô (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 16 ‡∏Å.‡∏û. - 15 ‡∏û.‡∏Ñ.)\n",
    "    if (month == 2 and day >= 16) or (month in [3, 4]) or (month == 5 and day <= 15):\n",
    "        return 'Summer'\n",
    "    # ‡∏§‡∏î‡∏π‡∏ù‡∏ô (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 16 ‡∏û.‡∏Ñ. - 15 ‡∏ï.‡∏Ñ.)\n",
    "    elif (month == 5 and day >= 16) or (month in [6, 7, 8, 9]) or (month == 10 and day <= 15):\n",
    "        return 'Rainy'\n",
    "    # ‡∏§‡∏î‡∏π‡∏´‡∏ô‡∏≤‡∏ß (‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠)\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "# --- 3. [‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏ù‡∏ô (‡∏à‡∏≤‡∏Å Cell 5) ---\n",
    "def process_rain_files_2023():\n",
    "    print(\"--- üöÄ [‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1] ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô 2023 ---\")\n",
    "    \n",
    "    # --- 3.1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° \"‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï\" (District Map) ---\n",
    "    print(\"   üó∫Ô∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà DCODE ...\")\n",
    "    try:\n",
    "        district_file_path = '../data/PROCESSED/overall_data.xlsx'\n",
    "        district_sheet_name = 'districts'\n",
    "        \n",
    "        df_districts = pd.read_excel(district_file_path, sheet_name=district_sheet_name)\n",
    "        df_district_map = df_districts[['dcode', 'dname']].copy()\n",
    "        \n",
    "        df_district_map['join_key'] = df_district_map['dname'].apply(normalize_district_name_v4_2)\n",
    "        df_district_map = df_district_map.drop_duplicates(subset=['join_key'])\n",
    "        \n",
    "        print(f\"   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà DCODE ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏°‡∏µ {len(df_district_map)} ‡πÄ‡∏Ç‡∏ï)\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î dcode map: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # --- 3.2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏ù‡∏ô 12 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (‡∏õ‡∏µ 2023) ---\n",
    "    print(\"\\n   üå¶Ô∏è ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô 2023 ...\")\n",
    "    \n",
    "    # [!!!] ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Path ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 [!!!]\n",
    "    rain_data_dir = '../data/RAW/rain_2023/'\n",
    "    \n",
    "    all_xlsx_files = glob.glob(os.path.join(rain_data_dir, '*.xlsx'))\n",
    "    rain_files = sorted(\n",
    "        [f for f in all_xlsx_files if not os.path.basename(f).startswith('~$')]\n",
    "    )\n",
    "\n",
    "    if not rain_files:\n",
    "        print(f\"   ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå .xlsx ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {rain_data_dir}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"   üìÇ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô {len(rain_files)} ‡πÑ‡∏ü‡∏•‡πå. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏ô‡∏•‡∏π‡∏õ...\")\n",
    "\n",
    "    all_processed_data = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        for file_path in rain_files:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            print(f\"      - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: {file_name}...\")\n",
    "            \n",
    "            dfs_by_day = pd.read_excel(file_path, sheet_name=None)\n",
    "            \n",
    "            for sheet_name, df_day in dfs_by_day.items():\n",
    "                if df_day.empty or '‡πÄ‡∏Ç‡∏ï' not in df_day.columns:\n",
    "                    continue\n",
    "                \n",
    "                df_day['join_key'] = df_day['‡πÄ‡∏Ç‡∏ï'].apply(normalize_district_name_v4_2)\n",
    "                df_merged = pd.merge(\n",
    "                    df_day, df_district_map, on='join_key', how='left'\n",
    "                )\n",
    "                all_processed_data.append(df_merged)\n",
    "            \n",
    "        print(\"      ... ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏±‡πâ‡∏á 12 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
    "\n",
    "        # --- 3.3. ‡∏£‡∏ß‡∏°‡∏£‡πà‡∏≤‡∏á (Concat) ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ---\n",
    "        if not all_processed_data:\n",
    "            print(\"   ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏î‡πÜ ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡πÄ‡∏•‡∏¢!\")\n",
    "            return False\n",
    "        \n",
    "        print(\"\\n   ...‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å 365+ ‡∏ä‡∏µ‡∏ï...\")\n",
    "        final_rain_df_raw = pd.concat(all_processed_data, ignore_index=True)\n",
    "        end_time = time.time()\n",
    "        print(f\"   ‚úÖ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ {end_time - start_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)\")\n",
    "        print(f\"      ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö (‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏£‡∏≠‡∏á): {final_rain_df_raw.shape[0]} ‡πÅ‡∏ñ‡∏ß\")\n",
    "\n",
    "        # --- 3.4. ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà (‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£) ‡∏≠‡∏≠‡∏Å ---\n",
    "        print(\"\\n   üóëÔ∏è ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà ‡∏Å‡∏ó‡∏°. ...\")\n",
    "        \n",
    "        final_rain_df = final_rain_df_raw[\n",
    "            final_rain_df_raw['‡πÄ‡∏Ç‡∏ï'] != '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£'\n",
    "        ].copy() \n",
    "        \n",
    "        removed_rows = len(final_rain_df_raw) - len(final_rain_df)\n",
    "        print(f\"   ‚úÖ ‡∏Å‡∏£‡∏≠‡∏á '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£' ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ {removed_rows} ‡πÅ‡∏ñ‡∏ß\")\n",
    "        print(f\"      ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡∏Å‡∏ó‡∏°.): {final_rain_df.shape[0]} ‡πÅ‡∏ñ‡∏ß\")\n",
    "\n",
    "        # --- 3.5. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡πÅ‡∏•‡∏∞ ‡πÅ‡∏õ‡∏•‡∏á DCODE ---\n",
    "        unmatched_count = final_rain_df['dcode'].isna().sum()\n",
    "        \n",
    "        if unmatched_count > 0:\n",
    "            print(f\"   ‚ö†Ô∏è ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏°‡∏µ {unmatched_count} ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏´‡∏≤ DCODE ‡πÑ‡∏°‡πà‡∏û‡∏ö (‡πÉ‡∏ô ‡∏Å‡∏ó‡∏°.)\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ DCODE ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡πÉ‡∏ô ‡∏Å‡∏ó‡∏°.) ‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!\")\n",
    "            try:\n",
    "                final_rain_df['dcode'] = final_rain_df['dcode'].astype(int)\n",
    "                print(\"   ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á dcode ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏° (int) ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
    "            except ValueError as e:\n",
    "                print(f\"   [!] ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á dcode ‡πÄ‡∏õ‡πá‡∏ô int ‡πÑ‡∏î‡πâ: {e}\")\n",
    "        \n",
    "        # --- 3.6. ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏•‡∏ö '‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•' ---\n",
    "        if 'dcode' in final_rain_df.columns:\n",
    "            key_cols = ['dcode', 'dname', '‡πÄ‡∏Ç‡∏ï', '‡∏£‡∏´‡∏±‡∏™‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ', '‡∏ß‡∏±‡∏ô-‡πÄ‡∏ß‡∏•‡∏≤']\n",
    "            other_cols = [\n",
    "                col for col in final_rain_df.columns \n",
    "                if col not in key_cols and col not in ['join_key', '‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•']\n",
    "            ]\n",
    "            final_rain_df_sorted = final_rain_df[key_cols + other_cols]\n",
    "            print(\"   [i] ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•' ‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "        else:\n",
    "            final_rain_df_sorted = final_rain_df\n",
    "\n",
    "        # --- 3.7. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå ---\n",
    "        output_dir = '../data/PROCESSED/'\n",
    "        \n",
    "        # [!!!] ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Output ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 [!!!]\n",
    "        output_filename = 'rain_2023_combined_bkk_only.csv' \n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n   üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1 ‡πÑ‡∏õ‡∏ó‡∏µ‡πà: {output_path}\")\n",
    "        final_rain_df_sorted.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(\"   üéâ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- 4. [‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏§‡∏î‡∏π (‡∏à‡∏≤‡∏Å Cell 11) ---\n",
    "def add_seasons_2023():\n",
    "    print(\"\\n--- üöÄ [‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2] ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏§‡∏î‡∏π 2023 ---\")\n",
    "    \n",
    "    # [!!!] Path Input/Output ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏µ 2023 [!!!]\n",
    "    rain_file_path = '../data/PROCESSED/rain_2023_combined_bkk_only.csv'\n",
    "    output_path = '../data/PROCESSED/rain_2023_with_seasons.csv'\n",
    "\n",
    "    print(f\"   - 1. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: {rain_file_path}\")\n",
    "    try:\n",
    "        rain_df = pd.read_csv(rain_file_path)\n",
    "        print(f\"      ... ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {len(rain_df)} ‡πÅ‡∏ñ‡∏ß\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"   !!! Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {rain_file_path}\")\n",
    "        print(\"   !!! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1 ‡πÉ‡∏´‡πâ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏Å‡πà‡∏≠‡∏ô\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"   ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- 4.1. [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡∏°‡πà] ‡∏ó‡∏≥ Feature Engineering (‡∏ã‡πà‡∏≠‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡∏û.‡∏®.) ---\n",
    "    print(\"   - 2. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡∏û.‡∏®. 2566 ‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ.‡∏®. 2023 ...\")\n",
    "    try:\n",
    "        date_str = rain_df['‡∏ß‡∏±‡∏ô-‡πÄ‡∏ß‡∏•‡∏≤'].astype(str)\n",
    "        \n",
    "        # [!!!!!!] ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô 2566 ‡πÄ‡∏õ‡πá‡∏ô 2023 [!!!!!!]\n",
    "        corrected_date_str = date_str.str.replace('2566', '2023')\n",
    "        \n",
    "        rain_df['date_object'] = pd.to_datetime(corrected_date_str, dayfirst=True, errors='coerce')\n",
    "        \n",
    "        original_rows = len(rain_df)\n",
    "        rain_df.dropna(subset=['date_object'], inplace=True)\n",
    "        dropped_rows = original_rows - len(rain_df)\n",
    "        \n",
    "        if dropped_rows > 0:\n",
    "            print(f\"      ... (‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô) ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ {dropped_rows} ‡πÅ‡∏ñ‡∏ß\")\n",
    "        print(\"      ... ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô datetime object (‡∏Ñ.‡∏®.) ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   !!! Error ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- 4.2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'season' ---\n",
    "    rain_df['season'] = rain_df['date_object'].apply(get_season)\n",
    "    print(\"   - 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'season' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
    "\n",
    "    # --- 4.3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà ---\n",
    "    print(f\"   - 4. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡∏ó‡∏µ‡πà {output_path} ...\")\n",
    "    rain_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(\"   üéâ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2 ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "    return True\n",
    "\n",
    "# --- 5. [‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏£‡∏ß‡∏°‡∏û‡∏≤‡∏¢‡∏∏ (‡∏à‡∏≤‡∏Å Cell 10) ---\n",
    "def merge_storm_data_2023():\n",
    "    print(\"\\n--- üöÄ [‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3] ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏ 2023 ---\")\n",
    "    \n",
    "    # [!!!] Path Input/Output ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏µ 2023 [!!!]\n",
    "    base_path = '../'\n",
    "    storm_file_path = os.path.join(base_path, 'data', 'RAW', 'stormtrack2023.xlsx')\n",
    "    rain_file_path = os.path.join(base_path, 'data', 'PROCESSED', 'rain_2023_with_seasons.csv')\n",
    "    \n",
    "    output_dir = os.path.join(base_path, 'data', 'PROCESSED')\n",
    "    output_filename = 'rain_with_storm_summary_2023.csv'\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    print(f\"   - ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏: {storm_file_path}\")\n",
    "    print(f\"   - ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô: {rain_file_path}\")\n",
    "\n",
    "    # --- 5.1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---\n",
    "    try:\n",
    "        storm_df = pd.read_excel(storm_file_path)\n",
    "        rain_df = pd.read_csv(rain_file_path)\n",
    "        print(\"\\n   ... ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
    "        print(f\"      ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô (2023): {rain_df.shape[0]} ‡πÅ‡∏ñ‡∏ß\")\n",
    "        print(f\"      ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏ (2023): {storm_df.shape[0]} ‡πÅ‡∏ñ‡∏ß\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"   [‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {e.filename}\")\n",
    "        print(\"   !!! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1 ‡πÅ‡∏•‡∏∞ 2 ‡πÉ‡∏´‡πâ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏Å‡πà‡∏≠‡∏ô\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"   [‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î] ‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- 5.2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏ (Storm Data) ---\n",
    "    print(\"\\n   ... ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô...\")\n",
    "    storm_df['yyyymmddhh'] = storm_df['yyyymmddhh'].astype(str)\n",
    "    storm_df['date_str'] = storm_df['yyyymmddhh'].str.slice(0, 8)\n",
    "    storm_df['date_object'] = pd.to_datetime(storm_df['date_str'], format='%Y%m%d').dt.strftime('%Y-%m-%d')\n",
    "    storm_df['stormname'] = storm_df['stormname'].str.strip().replace('', 'INVEST') \n",
    "\n",
    "    # --- 5.3. ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô (Aggregate Storm Data) ---\n",
    "    storm_summary_df = storm_df.groupby('date_object').agg(\n",
    "        active_storms_count=('stom_no', 'nunique'),\n",
    "        max_vmax_all_storms=('vmax', 'max'),\n",
    "        min_mlsp_all_storms=('mlsp', 'min'),\n",
    "        active_storm_names=('stormname', lambda x: list(x.unique()))\n",
    "    ).reset_index()\n",
    "    print(f\"   ... ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {storm_summary_df.shape[0]} ‡πÅ‡∏ñ‡∏ß (‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô)\")\n",
    "\n",
    "    # --- 5.4. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô (Rain Data) ---\n",
    "    rain_df['date_object'] = pd.to_datetime(rain_df['date_object']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # --- 5.5. ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Merge Data) ---\n",
    "    print(\"\\n   ... ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏ô‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≤‡∏¢‡∏∏...\")\n",
    "    merged_df = pd.merge(\n",
    "        rain_df,\n",
    "        storm_summary_df,\n",
    "        on='date_object',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # --- 5.6. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏π‡πà (Fill Missing Values) ---\n",
    "    merged_df['active_storms_count'] = merged_df['active_storms_count'].fillna(0).astype(int)\n",
    "    merged_df['max_vmax_all_storms'] = merged_df['max_vmax_all_storms'].fillna(0)\n",
    "    merged_df['min_mlsp_all_storms'] = merged_df['min_mlsp_all_storms'].fillna(np.nan) \n",
    "    merged_df['active_storm_names'] = merged_df['active_storm_names'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    print(f\"   ... ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏µ: {merged_df.shape[0]} ‡πÅ‡∏ñ‡∏ß\")\n",
    "\n",
    "    # --- 5.7. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå ---\n",
    "    try:\n",
    "        merged_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n   üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏ó‡∏µ‡πà: {output_path}\")\n",
    "        print(\"\\nüéâüéâüéâ [‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô] ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏µ 2023 ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! üéâüéâüéâ\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"   [‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î] ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- 6. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô ---\n",
    "def main():\n",
    "    try:\n",
    "        # ‡∏£‡∏±‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1\n",
    "        if process_rain_files_2023():\n",
    "            # ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2\n",
    "            if add_seasons_2023():\n",
    "                # ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2 ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3\n",
    "                merge_storm_data_2023()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå‚ùå‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡πÉ‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å: {e}\")\n",
    "        print(\"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Error ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flood_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
